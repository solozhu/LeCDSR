{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:37:16.783452Z",
     "start_time": "2025-04-09T14:37:16.779710Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "food_path = \"dataset/meta_Grocery_and_Gourmet_Food.json.gz\"\n",
    "kitchen_path = 'dataset/meta_Home_and_Kitchen.json.gz'\n",
    "\n",
    "movie_path = 'dataset/meta_Movies_and_TV.json.gz'\n",
    "book_path = 'dataset/meta_Books.json.gz'\n",
    "\n",
    "sport_path = 'dataset/meta_Sports_and_Outdoors_2018.json.gz'\n",
    "cloth_path = 'dataset/meta_Clothing_Shoes_and_Jewelry_2018.json.gz'\n",
    "\n",
    "paths = [food_path, kitchen_path, movie_path, book_path, sport_path, cloth_path]\n",
    "domains = ['Food', 'Kitchen', 'Movie', 'Book', 'Sport', 'Clothing']\n",
    "\n",
    "\n",
    "def get_item_list(file_path):\n",
    "    \"\"\"获取商品列表，商品id已设为id，使用iloc查询指定商品\"\"\"\n",
    "    # 逐行读取文件\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # 处理每一行\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        row = line.strip().split('\\t')  # 按制表符分割\n",
    "        data.append(row)\n",
    "    # 转换为DataFrame\n",
    "    column = ['origin_index', 'asin', 'id']\n",
    "    df = pd.DataFrame(data, columns=column)\n",
    "    df.set_index('id', inplace=True)\n",
    "    return df\n",
    "\n",
    "def get_user_list(file_path):\n",
    "    \"\"\"使用dataset/Food-Kitchen/userlist.txt, 按行读取每一个用户id\n",
    "    \"\"\"\n",
    "    # 逐行读取文件\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # 处理每一行\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        row = line.strip().split('\\t')  # 按制表符分割\n",
    "        data.append(row)\n",
    "    # 转换为DataFrame\n",
    "    column = ['asin', 'id']\n",
    "    df = pd.DataFrame(data, columns=column)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:37:16.788712Z",
     "start_time": "2025-04-09T14:37:16.786963Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_messing_value(df_item_list, df_meta_data):\n",
    "    df_item_list['isin_meta_data'] = df_item_list['asin'].isin(df_meta_data['asin'])\n",
    "    count = df_item_list['isin_meta_data'].value_counts()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:37:16.797352Z",
     "start_time": "2025-04-09T14:37:16.795784Z"
    }
   },
   "outputs": [],
   "source": [
    "# for i in range(len(domains)):\n",
    "#     item_path = f'dataset-single_domain\\{domains[i]}\\list.txt'\n",
    "#     df_item_list = get_item_list(item_path)\n",
    "#     df_meta_data = get_meta_data(paths[i])\n",
    "#     count = check_messing_value(df_item_list, df_meta_data)\n",
    "#     print(f'{domains[i]}:')\n",
    "#     print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:37:16.842367Z",
     "start_time": "2025-04-09T14:37:16.806836Z"
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import codecs\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='https://dashscope.aliyuncs.com/compatible-mode/v1',\n",
    "    api_key='sk-1bf40b7f0bdf41cf9e68509aa6dc1296'\n",
    ")\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def get_item_embedding(df, asin):\n",
    "    # 查询特定的asin\n",
    "    item = df[df[\"asin\"] == asin]\n",
    "    if not item.empty:\n",
    "        title = item['title'].item()\n",
    "        category = item['categories'].item()\n",
    "        description = item['description'].item()\n",
    "        text_desc = f'title: {title}\\ncategory: {category}\\ndescription: {description}'\n",
    "        if len(text_desc) >= 8192:\n",
    "            text_desc = f'title: {title}\\ncategory: {category}\\n'\n",
    "        response = client.embeddings.create(\n",
    "            input=text_desc,\n",
    "            model=\"text-embedding-v3\",\n",
    "            dimensions=1024,\n",
    "            encoding_format=\"float\"\n",
    "        )\n",
    "\n",
    "        return response.data[0].embedding\n",
    "    else:\n",
    "        return [0] * 1024\n",
    "    \n",
    "def get_user_embedding(user, df_meta_data, df_item, interactions):\n",
    "    \"\"\"使用用户交互序列直接生成用户画像\"\"\"\n",
    "    titles = []\n",
    "    str_list = []\n",
    "    text_desc = \"Below is a user's purchase record of items in the Book and Movie categories. Please note the temporal information: the higher the sequence number, the more recent the record. Use this sequence to characterize the user embedding.\\n\"\n",
    "    for id in interactions[user]:\n",
    "        asin = df_item.iloc[id]['asin']\n",
    "        item = df_meta_data[df_meta_data[\"asin\"] == asin]\n",
    "        if not item.empty:\n",
    "            title = item['title'].item()\n",
    "            titles.append(title)\n",
    "    num_tokens = len(encoding.encode(text_desc))\n",
    "    for i in range(len(titles)):\n",
    "        num_tokens += len(encoding.encode(f'{i+1}. {titles[i]}\\n'))\n",
    "        if num_tokens >= 6000:\n",
    "            print(text_desc)\n",
    "            str_list.append(text_desc)\n",
    "            if len(str_list) >= 10:\n",
    "                str_list = str_list[-10:]\n",
    "            text_desc = ''\n",
    "            num_tokens = 0\n",
    "        text_desc += f'{i+1}. {titles[i]}\\n'\n",
    "    str_list.append(text_desc)\n",
    "    print(f\"User: {user}, list length: {len(str_list)}\")\n",
    "    response = client.embeddings.create(\n",
    "        input=str_list,\n",
    "        model=\"text-embedding-v3\",\n",
    "        dimensions=1024,\n",
    "        encoding_format=\"float\"\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "    \n",
    "def split_string(input_string, max_length=8192):\n",
    "    str_list = [input_string[i:i + max_length] for i in range(0, len(input_string), max_length)]\n",
    "    return '\\n'.join(str_list)\n",
    "\n",
    "def read_train_data(train_file):\n",
    "    with codecs.open(train_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "        train_data = []\n",
    "        user = []\n",
    "        for id, line in enumerate(infile):\n",
    "            res = []\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            user.append(int(line[0]))\n",
    "\n",
    "            line = line[2:]  # 交互的一系列物品\n",
    "            for w in line:\n",
    "                w = w.split(\"|\")\n",
    "                res.append((int(w[0]), int(w[1])))\n",
    "            res.sort(key=lambda x: x[1])  # 按照时间顺序排列\n",
    "\n",
    "            res_2 = []\n",
    "            for r in res:\n",
    "                res_2.append(r[0])\n",
    "            train_data.append(res_2)\n",
    "\n",
    "    return train_data, user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:37:16.846115Z",
     "start_time": "2025-04-09T14:37:16.844407Z"
    }
   },
   "outputs": [],
   "source": [
    "# for i in range(len(domains)):\n",
    "#     file_path = f'dataset-single_domain\\{domains[i]}\\list.txt'\n",
    "#     df_item_list = get_item_list(file_path)\n",
    "#     df_meta_data = get_meta_data(paths[i])\n",
    "#     df_item_list['embedding'] = df_item_list['asin'].apply(lambda x: get_embedding(df_meta_data, x))\n",
    "#     df_item_list.to_csv(f'dataset/embedded_{domains[i]}.csv')\n",
    "#     print(f'{domains[i]} domain done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:37:16.853730Z",
     "start_time": "2025-04-09T14:37:16.850793Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_to_np_embeddings(domains, user_num):\n",
    "    \"\"\"将用户及商品嵌入转化为numpy文件，便于读取。处理过程中，仅对训练集中的用户生成了用户画像，而测试集与验证集中的用户没有用户画像，即冷启动问题\n",
    "    此处将缺失的用户画像补充为0向量，两个域的商品嵌入合并为一个列表\n",
    "    \"\"\"\n",
    "    domain_x = domains.split('-')[0]\n",
    "    domain_y = domains.split('-')[1]\n",
    "    # df_x = pd.read_csv(f'dataset/{domains}/embedded_{domain_x}.csv', index_col='index')\n",
    "    # df_y = pd.read_csv(f'dataset/{domains}/embedded_{domain_y}.csv', index_col='index')\n",
    "    df_user = pd.read_csv(f'dataset/{domains}/embedded_reasoning_user.csv')\n",
    "    # tqdm.pandas(desc=f\"Loading {domain_x} Embeddings\")\n",
    "    # df_x['embedding'] = df_x.embedding.progress_apply(lambda x: np.array(eval(x), dtype=np.float32))\n",
    "    # tqdm.pandas(desc=f\"Loading {domain_y} Embeddings\")\n",
    "    # df_y['embedding'] = df_y.embedding.progress_apply(lambda x: np.array(eval(x), dtype=np.float32))\n",
    "    tqdm.pandas(desc=f\"Loading User Embeddings\")\n",
    "    df_user['embedding'] = df_user.embedding.progress_apply(lambda x: np.array(eval(x), dtype=np.float32))\n",
    "    all_user_ids = list(range(0, user_num))\n",
    "    # 找出缺失的用户ID\n",
    "    missing_user_ids = [uid for uid in all_user_ids if uid not in df_user['id'].values]\n",
    "    # 创建一个包含缺失用户ID的补充数据框\n",
    "    supplement_df = pd.DataFrame({'id': missing_user_ids, 'embedding': [[0]*1024]*len(missing_user_ids)})\n",
    "    # 将补充的数据框与原始数据框合并\n",
    "    df_user = pd.concat([df_user, supplement_df]).sort_values(by='id').reset_index(drop=True)\n",
    "    # array_x = np.vstack(df_x['embedding'].to_numpy())\n",
    "    # array_y = np.vstack(df_y['embedding'].to_numpy())\n",
    "    # item_embeddings = np.vstack((array_x, array_y, np.zeros(1024, dtype=np.float32)))\n",
    "    user_embeddings = np.vstack(df_user['embedding'].to_numpy())\n",
    "    # np.save(f'dataset/{domains}/item_embeddings.npy', item_embeddings)\n",
    "    np.save(f'dataset/{domains}/reasoning_user_embeddings.npy', user_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:37:16.879257Z",
     "start_time": "2025-04-09T14:37:16.877849Z"
    }
   },
   "outputs": [],
   "source": [
    "# load_embeddings('Food-Kitchen', 16579)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:37:16.885652Z",
     "start_time": "2025-04-09T14:37:16.883977Z"
    }
   },
   "outputs": [],
   "source": [
    "save_to_np_embeddings('Movie-Book', 15352)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:47:33.218416Z",
     "start_time": "2025-04-09T15:47:31.864382Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_user_interaction(domains):\n",
    "    \"\"\"获取所有用户及其对应的用户交互\n",
    "    \"\"\"\n",
    "    # 从训练集中抽取所有用户交互，保留时间排序\n",
    "    train_file = f'dataset/{domains}/traindata_new.txt'\n",
    "    with codecs.open(train_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "            train_data = []\n",
    "            user_list = []\n",
    "            for id, line in enumerate(infile):\n",
    "                res = []\n",
    "                line = line.strip().split(\"\\t\")\n",
    "                user_list.append(int(line[0]))\n",
    "\n",
    "                line = line[2:]  # 交互的一系列物品\n",
    "                for w in line:\n",
    "                    w = w.split(\"|\")\n",
    "                    res.append((int(w[0]), int(w[1])))\n",
    "                res.sort(key=lambda x: x[1])  # 按照时间顺序排列\n",
    "\n",
    "                res_2 = []\n",
    "                for r in res:\n",
    "                    res_2.append(r[0])\n",
    "                train_data.append(res_2)\n",
    "    user_interaction = dict()\n",
    "    for user, data in zip(user_list, train_data):\n",
    "        user_interaction.setdefault(user, []).append(data)\n",
    "    for user in user_interaction.keys():\n",
    "        user_interaction[user] = list(dict.fromkeys(sum(user_interaction[user], [])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:49:47.795376Z",
     "start_time": "2025-04-09T15:47:35.615218Z"
    }
   },
   "outputs": [],
   "source": [
    "domain = 'Sport-Clothing'\n",
    "user_interaction = get_user_interaction(domain)\n",
    "file_path_A = f'dataset/{domain}/Alist.txt'\n",
    "file_path_B = f'dataset/{domain}/Blist.txt'\n",
    "df_item_Alist = get_item_list(file_path_A)\n",
    "df_item_Blist = get_item_list(file_path_B)\n",
    "# item数量直接加和为两域之和\n",
    "df_item = pd.concat([df_item_Alist, df_item_Blist])\n",
    "df_meta_data_A = get_meta_data(sport_path)\n",
    "df_meta_data_B = get_meta_data(cloth_path)\n",
    "df_meta_data = pd.concat([df_meta_data_A, df_meta_data_B])\n",
    "df_user = pd.DataFrame(user_interaction.keys(), columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm.pandas(desc=\"Getting User Embeddings\")\n",
    "# df_user['embedding'] = df_user['id'].apply(lambda x: get_user_embedding(x, df_meta_data, df_item, user_interaction))\n",
    "# df_user.to_csv('dataset/Movie-Book/embedded_user.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:49:49.661960Z",
     "start_time": "2025-04-09T15:49:47.889175Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_text = pd.merge(df_item, df_meta_data, on='asin', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df = df_text[df_text['id'].isin(user_interaction[7])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df = df_text.iloc[user_interaction[7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:49:49.668860Z",
     "start_time": "2025-04-09T15:49:49.666942Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_interaction_metadata(df_text, user_interaction):\n",
    "    text = ''\n",
    "    for num, id in enumerate(user_interaction):\n",
    "        item = df_text.iloc[id]\n",
    "        title = str(item['title'])\n",
    "        category = str(item['categories'])\n",
    "        text += f'{num+1}. ' + title + '  ' + category + '\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:50:04.727644Z",
     "start_time": "2025-04-09T15:50:01.668264Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def generate_request_data(custom_id, content):\n",
    "    \"\"\"\n",
    "    生成单个请求数据\n",
    "    :param custom_id: 自定义ID\n",
    "    :param content: 用户问题内容\n",
    "    :return: 请求数据的字典格式\n",
    "    \"\"\"\n",
    "    prompt = \"Below is a user's purchase record of items in the Toy-Game domains. Please note the temporal information: the higher the sequence number, the more recent the record. Use this sequence to characterize the user profile. The user profile should be concise and refined. And please avoid inappropriate content!\"\n",
    "    request_data = {\n",
    "        \"custom_id\": custom_id,\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"qwq-plus\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a recommendation expert, capable of profiling users based on their interaction records. The user profile should be concise and refined.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt+content}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    return request_data\n",
    "\n",
    "def generate_embedding_data(custom_id, content):\n",
    "    \"\"\"\n",
    "    生成单个请求数据\n",
    "    :param custom_id: 自定义ID\n",
    "    :param content: 用户问题内容\n",
    "    :return: 请求数据的字典格式\n",
    "    \"\"\"\n",
    "    prompt = \"Use this user profile to generate user embedding.\\n\"\n",
    "    request_data = {\n",
    "        \"custom_id\": custom_id,\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/embeddings\",\n",
    "        \"body\": {\n",
    "            \"model\": \"text-embedding-v3\",\n",
    "            \"input\": prompt+content,\n",
    "            \"encoding_format\": \"float\"\n",
    "        }\n",
    "    }\n",
    "    return request_data\n",
    "\n",
    "def save_to_json_file(data_list, file_name):\n",
    "    \"\"\"\n",
    "    将生成的数据列表保存为JSON文件\n",
    "    :param data_list: 数据列表\n",
    "    :param file_name: 保存的文件名\n",
    "    \"\"\"\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "        for data in data_list:\n",
    "            json.dump(data, file, ensure_ascii=False)\n",
    "            file.write(\"\\n\")  # 每条数据占一行\n",
    "\n",
    "def get_user_reasoning_embedding(profile):\n",
    "    \"\"\"使用用户交互序列推理用户画像并生成用户嵌入\"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        input=profile,\n",
    "        model=\"text-embedding-v3\",\n",
    "        dimensions=1024,\n",
    "        encoding_format=\"float\"\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_A = 'dataset/Movie-Book/Alist.txt'\n",
    "file_path_B = 'dataset/Movie-Book/Blist.txt'\n",
    "df_item_Alist = get_item_list(file_path_A)\n",
    "df_item_Blist = get_item_list(file_path_B)\n",
    "# item数量直接加和为两域之和\n",
    "df_item = pd.concat([df_item_Alist, df_item_Blist])\n",
    "df_meta_data_A = get_meta_data(movie_path)\n",
    "df_meta_data_B = get_meta_data(book_path)\n",
    "df_meta_data = pd.concat([df_meta_data_A, df_meta_data_B])\n",
    "df_text = pd.merge(df_item, df_meta_data, on='asin', how='left')\n",
    "\n",
    "# 从训练集中抽取所有用户交互，保留时间排序\n",
    "train_file = 'dataset/Movie-Book/traindata_new.txt'\n",
    "with codecs.open(train_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "        train_data = []\n",
    "        user_list = []\n",
    "        for id, line in enumerate(infile):\n",
    "            res = []\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            user_list.append(int(line[0]))\n",
    "\n",
    "            line = line[2:]  # 交互的一系列物品\n",
    "            for w in line:\n",
    "                w = w.split(\"|\")\n",
    "                res.append((int(w[0]), int(w[1])))\n",
    "            res.sort(key=lambda x: x[1])  # 按照时间顺序排列\n",
    "\n",
    "            res_2 = []\n",
    "            for r in res:\n",
    "                res_2.append(r[0])\n",
    "            train_data.append(res_2)\n",
    "user_interaction = dict()\n",
    "for user, data in zip(user_list, train_data):\n",
    "    user_interaction.setdefault(user, []).append(data)\n",
    "for user in user_interaction.keys():\n",
    "    user_interaction[user] = list(dict.fromkeys(sum(user_interaction[user], [])))\n",
    "# 生成用户画像推理文件\n",
    "data_list = []\n",
    "for id in user_interaction.keys():\n",
    "    data_list.append(generate_request_data(id, get_interaction_metadata(df_text, user_interaction[id])))\n",
    "\n",
    "save_to_json_file(data_list, \"movie-book_user_interaction.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_A = 'dataset/Movie-Book/Alist.txt'\n",
    "file_path_B = 'dataset/Movie-Book/Blist.txt'\n",
    "df_item_Alist = get_item_list(file_path_A)\n",
    "df_item_Blist = get_item_list(file_path_B)\n",
    "# item数量直接加和为两域之和\n",
    "df_item = pd.concat([df_item_Alist, df_item_Blist])\n",
    "df_meta_data_A = get_meta_data(movie_path)\n",
    "df_meta_data_B = get_meta_data(book_path)\n",
    "df_meta_data = pd.concat([df_meta_data_A, df_meta_data_B])\n",
    "df_text = pd.merge(df_item, df_meta_data, on='asin', how='left')\n",
    "\n",
    "# 从训练集中抽取所有用户交互，保留时间排序\n",
    "train_file = 'dataset/Movie-Book/traindata_new.txt'\n",
    "with codecs.open(train_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "        train_data = []\n",
    "        user_list = []\n",
    "        for id, line in enumerate(infile):\n",
    "            res = []\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            user_list.append(int(line[0]))\n",
    "\n",
    "            line = line[2:]  # 交互的一系列物品\n",
    "            for w in line:\n",
    "                w = w.split(\"|\")\n",
    "                res.append((int(w[0]), int(w[1])))\n",
    "            res.sort(key=lambda x: x[1])  # 按照时间顺序排列\n",
    "\n",
    "            res_2 = []\n",
    "            for r in res:\n",
    "                res_2.append(r[0])\n",
    "            train_data.append(res_2)\n",
    "user_interaction = dict()\n",
    "for user, data in zip(user_list, train_data):\n",
    "    user_interaction.setdefault(user, []).append(data)\n",
    "for user in user_interaction.keys():\n",
    "    user_interaction[user] = list(dict.fromkeys(sum(user_interaction[user], [])))\n",
    "# 生成用户画像推理文件\n",
    "data_list = []\n",
    "for id in user_interaction.keys():\n",
    "    # 使用原始id，确保交互列表中custom_id唯一\n",
    "    data_list.append(generate_request_data(id, get_interaction_metadata(df_text, user_interaction[id])))\n",
    "\n",
    "save_to_json_file(data_list, \"movie-book_user_interaction.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_interaction = dict()\n",
    "for user, data in zip(user_list, train_data):\n",
    "    user_interaction.setdefault(user, []).append(data)\n",
    "for user in user_interaction.keys():\n",
    "    user_interaction[user] = list(dict.fromkeys(sum(user_interaction[user], [])))\n",
    "# 生成用户画像推理文件\n",
    "data_list = []\n",
    "for id in user_interaction.keys():\n",
    "    # 使用原始id，确保交互列表中custom_id唯一\n",
    "    data_list.append(generate_request_data(id, get_interaction_metadata(df_text, user_interaction[id])))\n",
    "\n",
    "save_to_json_file(data_list, \"movie-book_user_interaction.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T07:23:33.941650Z",
     "start_time": "2025-04-10T07:23:33.792519Z"
    }
   },
   "outputs": [],
   "source": [
    "# 生成用户画像批处理文件\n",
    "file_path = 'dataset/Movie-Book/movie-book_user_profile.jsonl'\n",
    "\n",
    "# 打开文件并逐行读取\n",
    "data_list = []\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # 解析每一行的 JSON 数据\n",
    "        data = json.loads(line)\n",
    "        # 获取 custom_id 和 content\n",
    "        custom_id = data.get('custom_id')\n",
    "        content = data.get('response', {}).get('body', {}).get('choices', [{}])[0].get('message', {}).get('content', '')\n",
    "        data_list.append(generate_embedding_data(custom_id, content))\n",
    "\n",
    "save_to_json_file(data_list, \"dataset/Movie-Book/movie-book_embedding_request.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_string(input_string, max_length=5000):\n",
    "    \"\"\"\n",
    "    将输入字符串拆分为最大长度为max_length的字符串列表\n",
    "    :param input_string: 需要拆分的字符串\n",
    "    :param max_length: 每个子字符串的最大长度，默认为5000\n",
    "    :return: 拆分后的字符串列表\n",
    "    \"\"\"\n",
    "    return [input_string[i:i+max_length] for i in range(0, len(input_string), max_length)]\n",
    "\n",
    "\n",
    "def online_user_embeddings(domains):\n",
    "    \"\"\"\n",
    "    看起来不能使用批量推理生成嵌入，官方文档中对于能否使用text-embedding-v3模糊不清\n",
    "    \"\"\"\n",
    "    file_path = f'dataset/{domains}/{domains.lower()}_user_profile.jsonl'\n",
    "    # 打开文件并逐行读取\n",
    "    data_list = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # 解析每一行的 JSON 数据\n",
    "            data = json.loads(line)\n",
    "            # 获取 custom_id 和 content\n",
    "            custom_id = data.get('custom_id')\n",
    "            content = data.get('response', {}).get('body', {}).get('choices', [{}])[0].get('message', {}).get('content', '')\n",
    "            data_list.append((custom_id, split_string(content)))\n",
    "    df_user_profile = pd.DataFrame(data_list, columns=['id', 'user profile'])\n",
    "    tqdm.pandas(desc=\"Getting Reasoned User Embeddings\")\n",
    "    df_user_profile['embedding'] = df_user_profile['user profile'].progress_apply(lambda x: get_user_reasoning_embedding(x))\n",
    "    df_user_profile[['id', 'embedding']].to_csv(f'dataset/{domains}/embedded_reasoning_user.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_user_embeddings('Movie-Book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'dataset/Movie-Book/movie-book_user_profile.jsonl'\n",
    "# 打开文件并逐行读取\n",
    "data_list = []\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # 解析每一行的 JSON 数据\n",
    "        data = json.loads(line)\n",
    "        # 获取 custom_id 和 content\n",
    "        custom_id = data.get('custom_id')\n",
    "        content = data.get('response', {}).get('body', {}).get('choices', [{}])[0].get('message', {}).get('content', '')\n",
    "        data_list.append((custom_id, content))\n",
    "df_user_profile = pd.DataFrame(data_list, columns=['id', 'user profile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''**User Profile:**  \n",
    "A health-conscious, convenience-driven home cook with a growing interest in sleep comfort.  \n",
    "\n",
    "**Key Insights:**  \n",
    "1. **Home Cooking & Kitchen Tools**: Recent purchases (Toaster Oven Broiler, Spatter Cover) indicate a focus on kitchen appliances and gadgets for meal preparation.  \n",
    "2. **Snack-Oriented Diet**: Frequent buys of dried fruits, nuts, and ready-to-eat meals (GoPicnic, Tropical Flora, Dried Peaches) suggest a preference for convenient, portable, and possibly healthy snacks. Bulk purchases (e.g., 225-pack snacks) imply bulk buying for regular consumption.  \n",
    "3. **Bedding Comfort Upgrades**: Recent shifts toward cooling pillows and quilted pillows (most recent items) signal a priority for sleep comfort and ergonomic bedding solutions.  \n",
    "4. **Occasional Gifting**: Dried fruit trays may indicate occasional gifting or presentation-focused purchases.  \n",
    "\n",
    "**Behavioral Pattern**: Balances practicality (small appliances, snacks) with quality-of-life improvements (bedding upgrades), leaning toward convenience and health.\n",
    "\n",
    "'''\n",
    "\n",
    "print(text.replace('\\n', '\\\\n').replace('\"', '\\\\\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"User Profile:\\nPrimary Interests (Recent & Frequent Purchases):\\nCoffee Enthusiast: Regularly buys premium coffee (Senseo, Douwe Egberts) in pods and ground varieties, with a preference for decaf (items 1, 4, 6, 7, 9).\\nHome Organization & Furniture: Prioritizes functional storage solutions (bookshelves, garment racks, shelves) and ergonomic furniture (home office desks, bed elevators, folding bookcases) (items 3, 5, 10, 12, 14–15).\\nHome Comfort & Health: Invests in sleep/neck support (pillows, bed elevators) and maintenance (vacuum filters, can opener) (items 2, 5, 8, 11).\\nSecondary Interests (Occasional/Decor):\\nHome Décor: Adds aesthetic touches like posters, floating shelves, and themed shower curtains (items 13, 16).\\nOn-the-Go Essentials: Uses travel mugs (item 17), suggesting a need for portable convenience.\\nTemporal Trends:\\nRecent purchases (last 3 items) emphasize functional decor (shelf, shower curtain, travel mug), indicating a focus on practical aesthetics.\\nConsistent coffee buying reflects a stable routine, while home storage/organization items suggest ongoing efforts to optimize living/working spaces.\\nKey Traits:\\nEfficiency-Oriented: Prefers ready-to-use products (coffee pods, pre-packaged items).\\nErgonomic Awareness: Chooses products for comfort and health (neck pillows, bed elevators).\\nBrand Loyal: Repeats purchases from trusted brands (Senseo, Douwe Egberts, Umbra).\\nRefined Summary:\\nA coffee-loving, home-organization-focused individual optimizing their living/working spaces with ergonomic furniture, functional storage, and subtle decor, while maintaining routines with preferred coffee brands. Recent activity highlights a blend of practicality and aesthetic upgrades.\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理Toy-Game数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"处理用户交互记录\"\"\"\n",
    "columns = ['asin', 'reviewerID', 'rating', 'timestamp']\n",
    "user_log_toy = pd.read_csv('dataset/generate_LLM_embeddings\\Toy-Game\\Toys_and_Games.csv', names=columns)\n",
    "user_log_game = pd.read_csv('dataset/generate_LLM_embeddings\\Toy-Game\\Video_Games.csv', names=columns)\n",
    "# 将时间戳转换为日期时间\n",
    "user_log_toy['date'] = pd.to_datetime(user_log_toy['timestamp'], unit='s')\n",
    "user_log_game['date'] = pd.to_datetime(user_log_game['timestamp'], unit='s')\n",
    "\n",
    "# 定义2014年的开始时间\n",
    "start_of_2014 = datetime.datetime(2014, 1, 1)\n",
    "\n",
    "# 筛选2014年以后的数据\n",
    "user_log_toy = user_log_toy[user_log_toy['date'] >= start_of_2014]\n",
    "user_log_game = user_log_game[user_log_game['date'] >= start_of_2014]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0020232233</td>\n",
       "      <td>A1IDMI31WEANAF</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1474502400</td>\n",
       "      <td>2016-09-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0020232233</td>\n",
       "      <td>A4BCEVVZ4Y3V3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1474156800</td>\n",
       "      <td>2016-09-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0020232233</td>\n",
       "      <td>A2EZ9PY1IHHBX0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1473638400</td>\n",
       "      <td>2016-09-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0020232233</td>\n",
       "      <td>A139PXTTC2LGHZ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1488412800</td>\n",
       "      <td>2017-03-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0020232233</td>\n",
       "      <td>A3IB33V29XIL8O</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1486512000</td>\n",
       "      <td>2017-02-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8201226</th>\n",
       "      <td>B01HJBAKIO</td>\n",
       "      <td>A3OCDEVI6FGUWU</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1512604800</td>\n",
       "      <td>2017-12-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8201227</th>\n",
       "      <td>B01HJHA7GI</td>\n",
       "      <td>A1KTVUVADLKWZO</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1453507200</td>\n",
       "      <td>2016-01-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8201228</th>\n",
       "      <td>B01HJHA7GI</td>\n",
       "      <td>A2QCA9OE62IPZ4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1423353600</td>\n",
       "      <td>2015-02-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8201229</th>\n",
       "      <td>B01HJHA7GI</td>\n",
       "      <td>A3N28JAZYS4L9O</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1419984000</td>\n",
       "      <td>2014-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8201230</th>\n",
       "      <td>B01HJHA7GI</td>\n",
       "      <td>A2OGYZRYRSVQZJ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1415664000</td>\n",
       "      <td>2014-11-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6878404 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               asin      reviewerID  rating   timestamp       date\n",
       "0        0020232233  A1IDMI31WEANAF     2.0  1474502400 2016-09-22\n",
       "1        0020232233   A4BCEVVZ4Y3V3     1.0  1474156800 2016-09-18\n",
       "2        0020232233  A2EZ9PY1IHHBX0     3.0  1473638400 2016-09-12\n",
       "3        0020232233  A139PXTTC2LGHZ     5.0  1488412800 2017-03-02\n",
       "4        0020232233  A3IB33V29XIL8O     1.0  1486512000 2017-02-08\n",
       "...             ...             ...     ...         ...        ...\n",
       "8201226  B01HJBAKIO  A3OCDEVI6FGUWU     5.0  1512604800 2017-12-07\n",
       "8201227  B01HJHA7GI  A1KTVUVADLKWZO     5.0  1453507200 2016-01-23\n",
       "8201228  B01HJHA7GI  A2QCA9OE62IPZ4     5.0  1423353600 2015-02-08\n",
       "8201229  B01HJHA7GI  A3N28JAZYS4L9O     5.0  1419984000 2014-12-31\n",
       "8201230  B01HJHA7GI  A2OGYZRYRSVQZJ     5.0  1415664000 2014-11-11\n",
       "\n",
       "[6878404 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_log_toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_15388\\490473137.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped_toy = user_log_toy.groupby('reviewerID').apply(lambda x: [\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_15388\\490473137.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped_game = user_log_game.groupby('reviewerID').apply(lambda x: [\n"
     ]
    }
   ],
   "source": [
    "# 对 user_log_toy 按 reviewerID 分组\n",
    "grouped_toy = user_log_toy.groupby('reviewerID').apply(lambda x: [\n",
    "    {'asin': asin, 'rating': rating, 'timestamp': timestamp}\n",
    "    for asin, rating, timestamp in zip(x['asin'], x['rating'], x['timestamp'])\n",
    "]).reset_index(name='data_toy')\n",
    "\n",
    "# 对 user_log_game 按 reviewerID 分组\n",
    "grouped_game = user_log_game.groupby('reviewerID').apply(lambda x: [\n",
    "    {'asin': asin, 'rating': rating, 'timestamp': timestamp}\n",
    "    for asin, rating, timestamp in zip(x['asin'], x['rating'], x['timestamp'])\n",
    "]).reset_index(name='data_game')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>data_toy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0001528BGUBOEVR6T5U</td>\n",
       "      <td>[{'asin': 'B0019PU8XE', 'rating': 5.0, 'timest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>A0010158IH80M4C0LOJ1</td>\n",
       "      <td>[{'asin': 'B00PHQG6GY', 'rating': 5.0, 'timest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A001170867ZBE9FORRQL</td>\n",
       "      <td>[{'asin': 'B00H8OB6E0', 'rating': 5.0, 'timest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>A00222906VX8GH7X6J6B</td>\n",
       "      <td>[{'asin': 'B000A42YLY', 'rating': 5.0, 'timest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>A0022678B6GE9F3FOSBS</td>\n",
       "      <td>[{'asin': 'B005G14SGU', 'rating': 5.0, 'timest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3592100</th>\n",
       "      <td>AZZYVIRS854I7</td>\n",
       "      <td>[{'asin': 'B007EA4UBY', 'rating': 5.0, 'timest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3592102</th>\n",
       "      <td>AZZYW4YOE1B6E</td>\n",
       "      <td>[{'asin': 'B004S6DV2G', 'rating': 5.0, 'timest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3592107</th>\n",
       "      <td>AZZZ6G9WZTNNX</td>\n",
       "      <td>[{'asin': 'B000IBPD76', 'rating': 5.0, 'timest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3592124</th>\n",
       "      <td>AZZZYAYJQSDOJ</td>\n",
       "      <td>[{'asin': 'B00EBUVCW0', 'rating': 5.0, 'timest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3592125</th>\n",
       "      <td>AZZZZS162JNL0</td>\n",
       "      <td>[{'asin': 'B000EUJOIK', 'rating': 5.0, 'timest...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>588114 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   reviewerID  \\\n",
       "3        A0001528BGUBOEVR6T5U   \n",
       "14       A0010158IH80M4C0LOJ1   \n",
       "15       A001170867ZBE9FORRQL   \n",
       "28       A00222906VX8GH7X6J6B   \n",
       "29       A0022678B6GE9F3FOSBS   \n",
       "...                       ...   \n",
       "3592100         AZZYVIRS854I7   \n",
       "3592102         AZZYW4YOE1B6E   \n",
       "3592107         AZZZ6G9WZTNNX   \n",
       "3592124         AZZZYAYJQSDOJ   \n",
       "3592125         AZZZZS162JNL0   \n",
       "\n",
       "                                                  data_toy  \n",
       "3        [{'asin': 'B0019PU8XE', 'rating': 5.0, 'timest...  \n",
       "14       [{'asin': 'B00PHQG6GY', 'rating': 5.0, 'timest...  \n",
       "15       [{'asin': 'B00H8OB6E0', 'rating': 5.0, 'timest...  \n",
       "28       [{'asin': 'B000A42YLY', 'rating': 5.0, 'timest...  \n",
       "29       [{'asin': 'B005G14SGU', 'rating': 5.0, 'timest...  \n",
       "...                                                    ...  \n",
       "3592100  [{'asin': 'B007EA4UBY', 'rating': 5.0, 'timest...  \n",
       "3592102  [{'asin': 'B004S6DV2G', 'rating': 5.0, 'timest...  \n",
       "3592107  [{'asin': 'B000IBPD76', 'rating': 5.0, 'timest...  \n",
       "3592124  [{'asin': 'B00EBUVCW0', 'rating': 5.0, 'timest...  \n",
       "3592125  [{'asin': 'B000EUJOIK', 'rating': 5.0, 'timest...  \n",
       "\n",
       "[588114 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>data_game</th>\n",
       "      <th>data_toy</th>\n",
       "      <th>all_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0059486XI1Z0P98KP35</td>\n",
       "      <td>[{'asin': 'B000WE8JES', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B002QI4LYK', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B002QI4LYK', 'rating': 5.0, 'timest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0163982I33BFLFLDW0T</td>\n",
       "      <td>[{'asin': 'B002EE4VQY', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B008RDZCS2', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B008RDZCS2', 'rating': 5.0, 'timest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0220159ZRNBTRKLG08H</td>\n",
       "      <td>[{'asin': 'B000084318', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B00U9UB74O', 'rating': 3.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B00U9UB74O', 'rating': 3.0, 'timest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0266076X6KPZ6CCHGVS</td>\n",
       "      <td>[{'asin': 'B00003OTI3', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B00C2P72J8', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B00C2P72J8', 'rating': 5.0, 'timest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A02836981FYG9912C66F</td>\n",
       "      <td>[{'asin': 'B00JIJUB7G', 'rating': 4.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B01AW1R5XQ', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B01AW1R5XQ', 'rating': 5.0, 'timest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24967</th>\n",
       "      <td>AZZ1KF8RAO1BR</td>\n",
       "      <td>[{'asin': 'B00BMFIXOW', 'rating': 2.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B001B1VJJI', 'rating': 1.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B001B1VJJI', 'rating': 1.0, 'timest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24968</th>\n",
       "      <td>AZZJBKJX833IV</td>\n",
       "      <td>[{'asin': 'B00D7823Q6', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B000VLXDAC', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B000VLXDAC', 'rating': 5.0, 'timest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24969</th>\n",
       "      <td>AZZRS2XK17RFQ</td>\n",
       "      <td>[{'asin': 'B00BT2BFKW', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B0001XNTJA', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B0001XNTJA', 'rating': 5.0, 'timest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24970</th>\n",
       "      <td>AZZT1ERHBSNQ8</td>\n",
       "      <td>[{'asin': 'B00BU3ZLJQ', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B00004TFT1', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B00004TFT1', 'rating': 5.0, 'timest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24971</th>\n",
       "      <td>AZZV9M0S13U26</td>\n",
       "      <td>[{'asin': 'B0015AARJI', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B0076TRVGO', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B0076TRVGO', 'rating': 5.0, 'timest...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24972 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 reviewerID  \\\n",
       "0      A0059486XI1Z0P98KP35   \n",
       "1      A0163982I33BFLFLDW0T   \n",
       "2      A0220159ZRNBTRKLG08H   \n",
       "3      A0266076X6KPZ6CCHGVS   \n",
       "4      A02836981FYG9912C66F   \n",
       "...                     ...   \n",
       "24967         AZZ1KF8RAO1BR   \n",
       "24968         AZZJBKJX833IV   \n",
       "24969         AZZRS2XK17RFQ   \n",
       "24970         AZZT1ERHBSNQ8   \n",
       "24971         AZZV9M0S13U26   \n",
       "\n",
       "                                               data_game  \\\n",
       "0      [{'asin': 'B000WE8JES', 'rating': 5.0, 'timest...   \n",
       "1      [{'asin': 'B002EE4VQY', 'rating': 5.0, 'timest...   \n",
       "2      [{'asin': 'B000084318', 'rating': 5.0, 'timest...   \n",
       "3      [{'asin': 'B00003OTI3', 'rating': 5.0, 'timest...   \n",
       "4      [{'asin': 'B00JIJUB7G', 'rating': 4.0, 'timest...   \n",
       "...                                                  ...   \n",
       "24967  [{'asin': 'B00BMFIXOW', 'rating': 2.0, 'timest...   \n",
       "24968  [{'asin': 'B00D7823Q6', 'rating': 5.0, 'timest...   \n",
       "24969  [{'asin': 'B00BT2BFKW', 'rating': 5.0, 'timest...   \n",
       "24970  [{'asin': 'B00BU3ZLJQ', 'rating': 5.0, 'timest...   \n",
       "24971  [{'asin': 'B0015AARJI', 'rating': 5.0, 'timest...   \n",
       "\n",
       "                                                data_toy  \\\n",
       "0      [{'asin': 'B002QI4LYK', 'rating': 5.0, 'timest...   \n",
       "1      [{'asin': 'B008RDZCS2', 'rating': 5.0, 'timest...   \n",
       "2      [{'asin': 'B00U9UB74O', 'rating': 3.0, 'timest...   \n",
       "3      [{'asin': 'B00C2P72J8', 'rating': 5.0, 'timest...   \n",
       "4      [{'asin': 'B01AW1R5XQ', 'rating': 5.0, 'timest...   \n",
       "...                                                  ...   \n",
       "24967  [{'asin': 'B001B1VJJI', 'rating': 1.0, 'timest...   \n",
       "24968  [{'asin': 'B000VLXDAC', 'rating': 5.0, 'timest...   \n",
       "24969  [{'asin': 'B0001XNTJA', 'rating': 5.0, 'timest...   \n",
       "24970  [{'asin': 'B00004TFT1', 'rating': 5.0, 'timest...   \n",
       "24971  [{'asin': 'B0076TRVGO', 'rating': 5.0, 'timest...   \n",
       "\n",
       "                                                all_data  \n",
       "0      [{'asin': 'B002QI4LYK', 'rating': 5.0, 'timest...  \n",
       "1      [{'asin': 'B008RDZCS2', 'rating': 5.0, 'timest...  \n",
       "2      [{'asin': 'B00U9UB74O', 'rating': 3.0, 'timest...  \n",
       "3      [{'asin': 'B00C2P72J8', 'rating': 5.0, 'timest...  \n",
       "4      [{'asin': 'B01AW1R5XQ', 'rating': 5.0, 'timest...  \n",
       "...                                                  ...  \n",
       "24967  [{'asin': 'B001B1VJJI', 'rating': 1.0, 'timest...  \n",
       "24968  [{'asin': 'B000VLXDAC', 'rating': 5.0, 'timest...  \n",
       "24969  [{'asin': 'B0001XNTJA', 'rating': 5.0, 'timest...  \n",
       "24970  [{'asin': 'B00004TFT1', 'rating': 5.0, 'timest...  \n",
       "24971  [{'asin': 'B0076TRVGO', 'rating': 5.0, 'timest...  \n",
       "\n",
       "[24972 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 确保两个域的交互记录在3条以上\n",
    "grouped_toy = grouped_toy[grouped_toy['data_toy'].apply(lambda x: len(x) >= 3)]\n",
    "grouped_game = grouped_game[grouped_game['data_game'].apply(lambda x: len(x) >= 3)]\n",
    "\n",
    "# 合并两个分组结果\n",
    "merged_data = pd.merge(grouped_game, grouped_toy, on='reviewerID', how='inner')\n",
    "merged_data['all_data'] = merged_data.apply(lambda x: x['data_toy']+x['data_game'], axis=1)\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_15388\\1850550926.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['length'] = filtered_data['all_data'].apply(len)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20.963336329780404"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 过滤掉交互记录长度小于10的用户\n",
    "filtered_data = merged_data[merged_data['all_data'].apply(lambda x: len(x) >= 10)]\n",
    "\n",
    "# 计算每个用户的交互记录长度\n",
    "filtered_data['length'] = filtered_data['all_data'].apply(len)\n",
    "\n",
    "# 计算交互记录的平均长度\n",
    "average_length = filtered_data['length'].mean()\n",
    "average_length # = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>data_game</th>\n",
       "      <th>data_toy</th>\n",
       "      <th>all_data</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0163982I33BFLFLDW0T</td>\n",
       "      <td>[{'asin': 'B002EE4VQY', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B008RDZCS2', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B008RDZCS2', 'rating': 5.0, 'timest...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0266076X6KPZ6CCHGVS</td>\n",
       "      <td>[{'asin': 'B00003OTI3', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B00C2P72J8', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B00C2P72J8', 'rating': 5.0, 'timest...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A02836981FYG9912C66F</td>\n",
       "      <td>[{'asin': 'B00JIJUB7G', 'rating': 4.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B01AW1R5XQ', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B01AW1R5XQ', 'rating': 5.0, 'timest...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A0422204VM5KZUEMVY96</td>\n",
       "      <td>[{'asin': 'B00C7103DO', 'rating': 3.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B000BMV0HE', 'rating': 3.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B000BMV0HE', 'rating': 3.0, 'timest...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A049248150WLX2UGA57G</td>\n",
       "      <td>[{'asin': 'B00DBDPOZ4', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B007XLEKW8', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B007XLEKW8', 'rating': 5.0, 'timest...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24964</th>\n",
       "      <td>AZY7LNNZZE02P</td>\n",
       "      <td>[{'asin': 'B00DB9JV5W', 'rating': 3.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B019NX6UMQ', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B019NX6UMQ', 'rating': 5.0, 'timest...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24966</th>\n",
       "      <td>AZYU8M791SIFC</td>\n",
       "      <td>[{'asin': 'B0000296O5', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B005KY1KY6', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B005KY1KY6', 'rating': 5.0, 'timest...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24968</th>\n",
       "      <td>AZZJBKJX833IV</td>\n",
       "      <td>[{'asin': 'B00D7823Q6', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B000VLXDAC', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B000VLXDAC', 'rating': 5.0, 'timest...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24970</th>\n",
       "      <td>AZZT1ERHBSNQ8</td>\n",
       "      <td>[{'asin': 'B00BU3ZLJQ', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B00004TFT1', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B00004TFT1', 'rating': 5.0, 'timest...</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24971</th>\n",
       "      <td>AZZV9M0S13U26</td>\n",
       "      <td>[{'asin': 'B0015AARJI', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B0076TRVGO', 'rating': 5.0, 'timest...</td>\n",
       "      <td>[{'asin': 'B0076TRVGO', 'rating': 5.0, 'timest...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15574 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 reviewerID  \\\n",
       "1      A0163982I33BFLFLDW0T   \n",
       "3      A0266076X6KPZ6CCHGVS   \n",
       "4      A02836981FYG9912C66F   \n",
       "6      A0422204VM5KZUEMVY96   \n",
       "7      A049248150WLX2UGA57G   \n",
       "...                     ...   \n",
       "24964         AZY7LNNZZE02P   \n",
       "24966         AZYU8M791SIFC   \n",
       "24968         AZZJBKJX833IV   \n",
       "24970         AZZT1ERHBSNQ8   \n",
       "24971         AZZV9M0S13U26   \n",
       "\n",
       "                                               data_game  \\\n",
       "1      [{'asin': 'B002EE4VQY', 'rating': 5.0, 'timest...   \n",
       "3      [{'asin': 'B00003OTI3', 'rating': 5.0, 'timest...   \n",
       "4      [{'asin': 'B00JIJUB7G', 'rating': 4.0, 'timest...   \n",
       "6      [{'asin': 'B00C7103DO', 'rating': 3.0, 'timest...   \n",
       "7      [{'asin': 'B00DBDPOZ4', 'rating': 5.0, 'timest...   \n",
       "...                                                  ...   \n",
       "24964  [{'asin': 'B00DB9JV5W', 'rating': 3.0, 'timest...   \n",
       "24966  [{'asin': 'B0000296O5', 'rating': 5.0, 'timest...   \n",
       "24968  [{'asin': 'B00D7823Q6', 'rating': 5.0, 'timest...   \n",
       "24970  [{'asin': 'B00BU3ZLJQ', 'rating': 5.0, 'timest...   \n",
       "24971  [{'asin': 'B0015AARJI', 'rating': 5.0, 'timest...   \n",
       "\n",
       "                                                data_toy  \\\n",
       "1      [{'asin': 'B008RDZCS2', 'rating': 5.0, 'timest...   \n",
       "3      [{'asin': 'B00C2P72J8', 'rating': 5.0, 'timest...   \n",
       "4      [{'asin': 'B01AW1R5XQ', 'rating': 5.0, 'timest...   \n",
       "6      [{'asin': 'B000BMV0HE', 'rating': 3.0, 'timest...   \n",
       "7      [{'asin': 'B007XLEKW8', 'rating': 5.0, 'timest...   \n",
       "...                                                  ...   \n",
       "24964  [{'asin': 'B019NX6UMQ', 'rating': 5.0, 'timest...   \n",
       "24966  [{'asin': 'B005KY1KY6', 'rating': 5.0, 'timest...   \n",
       "24968  [{'asin': 'B000VLXDAC', 'rating': 5.0, 'timest...   \n",
       "24970  [{'asin': 'B00004TFT1', 'rating': 5.0, 'timest...   \n",
       "24971  [{'asin': 'B0076TRVGO', 'rating': 5.0, 'timest...   \n",
       "\n",
       "                                                all_data  length  \n",
       "1      [{'asin': 'B008RDZCS2', 'rating': 5.0, 'timest...      15  \n",
       "3      [{'asin': 'B00C2P72J8', 'rating': 5.0, 'timest...      12  \n",
       "4      [{'asin': 'B01AW1R5XQ', 'rating': 5.0, 'timest...      10  \n",
       "6      [{'asin': 'B000BMV0HE', 'rating': 3.0, 'timest...      21  \n",
       "7      [{'asin': 'B007XLEKW8', 'rating': 5.0, 'timest...      11  \n",
       "...                                                  ...     ...  \n",
       "24964  [{'asin': 'B019NX6UMQ', 'rating': 5.0, 'timest...      18  \n",
       "24966  [{'asin': 'B005KY1KY6', 'rating': 5.0, 'timest...      21  \n",
       "24968  [{'asin': 'B000VLXDAC', 'rating': 5.0, 'timest...      12  \n",
       "24970  [{'asin': 'B00004TFT1', 'rating': 5.0, 'timest...      79  \n",
       "24971  [{'asin': 'B0076TRVGO', 'rating': 5.0, 'timest...      13  \n",
       "\n",
       "[15574 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取所有唯一的 reviewerID 和 asin\n",
    "unique_reviewerIDs = pd.Series(filtered_data['reviewerID'].unique())\n",
    "toy_unique_asins = pd.Series([item['asin'] for sublist in filtered_data['data_toy'] for item in sublist]).unique()\n",
    "game_unique_asins = pd.Series([item['asin'] for sublist in filtered_data['data_game'] for item in sublist]).unique()\n",
    "# 创建 reviewerID 和 asin 的映射字典\n",
    "reviewerID_map = {reviewerID: idx for idx, reviewerID in enumerate(unique_reviewerIDs)}\n",
    "toy_asin_map = {asin: idx for idx, asin in enumerate(toy_unique_asins)}\n",
    "game_asin_map = {asin: idx for idx, asin in enumerate(game_unique_asins)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15574, 86491, 22832)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviewerID_map), len(toy_asin_map), len(game_asin_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_15388\\1754520385.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['reviewerID_int'] = filtered_data['reviewerID'].map(reviewerID_map)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_15388\\1754520385.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['toy_data_int'] = filtered_data['data_toy'].apply(\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_15388\\1754520385.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['game_data_int'] = filtered_data['data_game'].apply(\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_15388\\1754520385.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['all_data_int'] = filtered_data.apply(lambda x: x['toy_data_int']+x['game_data_int'], axis=1)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_15388\\1754520385.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['all_data_sorted'] = filtered_data['all_data_int'].apply(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID_int</th>\n",
       "      <th>all_data_sorted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'asin_int': 0, 'rating': 5.0, 'timestamp': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'asin_int': 12, 'rating': 5.0, 'timestamp': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>[{'asin_int': 86505, 'rating': 3.0, 'timestamp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>[{'asin_int': 30, 'rating': 5.0, 'timestamp': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>[{'asin_int': 40, 'rating': 5.0, 'timestamp': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24964</th>\n",
       "      <td>15569</td>\n",
       "      <td>[{'asin_int': 5571, 'rating': 4.0, 'timestamp'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24966</th>\n",
       "      <td>15570</td>\n",
       "      <td>[{'asin_int': 86456, 'rating': 5.0, 'timestamp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24968</th>\n",
       "      <td>15571</td>\n",
       "      <td>[{'asin_int': 6001, 'rating': 5.0, 'timestamp'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24970</th>\n",
       "      <td>15572</td>\n",
       "      <td>[{'asin_int': 77004, 'rating': 5.0, 'timestamp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24971</th>\n",
       "      <td>15573</td>\n",
       "      <td>[{'asin_int': 62773, 'rating': 5.0, 'timestamp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15574 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID_int                                    all_data_sorted\n",
       "1                   0  [{'asin_int': 0, 'rating': 5.0, 'timestamp': 1...\n",
       "3                   1  [{'asin_int': 12, 'rating': 5.0, 'timestamp': ...\n",
       "4                   2  [{'asin_int': 86505, 'rating': 3.0, 'timestamp...\n",
       "6                   3  [{'asin_int': 30, 'rating': 5.0, 'timestamp': ...\n",
       "7                   4  [{'asin_int': 40, 'rating': 5.0, 'timestamp': ...\n",
       "...               ...                                                ...\n",
       "24964           15569  [{'asin_int': 5571, 'rating': 4.0, 'timestamp'...\n",
       "24966           15570  [{'asin_int': 86456, 'rating': 5.0, 'timestamp...\n",
       "24968           15571  [{'asin_int': 6001, 'rating': 5.0, 'timestamp'...\n",
       "24970           15572  [{'asin_int': 77004, 'rating': 5.0, 'timestamp...\n",
       "24971           15573  [{'asin_int': 62773, 'rating': 5.0, 'timestamp...\n",
       "\n",
       "[15574 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将 reviewerID 和 asin 映射为整数\n",
    "filtered_data['reviewerID_int'] = filtered_data['reviewerID'].map(reviewerID_map)\n",
    "filtered_data['toy_data_int'] = filtered_data['data_toy'].apply(\n",
    "    lambda x: [{'asin_int': toy_asin_map[item['asin']], 'rating': item['rating'], 'timestamp': item['timestamp']} for item in x]\n",
    ")\n",
    "filtered_data['game_data_int'] = filtered_data['data_game'].apply(\n",
    "    lambda x: [{'asin_int': game_asin_map[item['asin']]+len(toy_asin_map), 'rating': item['rating'], 'timestamp': item['timestamp']} for item in x]\n",
    ")\n",
    "filtered_data['all_data_int'] = filtered_data.apply(lambda x: x['toy_data_int']+x['game_data_int'], axis=1)\n",
    "# 按时间戳排序\n",
    "filtered_data['all_data_sorted'] = filtered_data['all_data_int'].apply(\n",
    "    lambda x: sorted(x, key=lambda k: k['timestamp'])\n",
    ")\n",
    "# 查看转换后的数据\n",
    "filtered_data[['reviewerID_int', 'all_data_sorted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data[['reviewerID_int', 'all_data_sorted']].to_csv(\"dataset/generate_LLM_embeddings/Toy-Game/megered_Toy-Game.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewerID_map_df = pd.DataFrame(list(reviewerID_map.items()), columns=['reviewerID', 'reviewerID_index'])\n",
    "toy_asin_map_df = pd.DataFrame(list(toy_asin_map.items()), columns=['asin', 'asin_index'])\n",
    "game_asin_map_df = pd.DataFrame(list(game_asin_map.items()), columns=['asin', 'asin_index'])\n",
    "reviewerID_map_df.to_csv('dataset/Toy-Game/user_list.csv', index=False)\n",
    "toy_asin_map_df.to_csv('dataset/Toy-Game/Alist.csv', index=False)\n",
    "game_asin_map_df.to_csv('dataset/Toy-Game/Blist.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# 拆分为训练集、验证集和测试集\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "train_data = []\n",
    "val_data = []\n",
    "test_data = []\n",
    "\n",
    "for _, row in filtered_data.iterrows():\n",
    "    sequence = row['all_data_sorted']\n",
    "    # 确保序列长度至少为3，以便拆分\n",
    "    if len(sequence) >=3:\n",
    "        # 按比例拆分\n",
    "        train_end = int(len(sequence) * train_ratio)\n",
    "        val_end = int(len(sequence) * (train_ratio + val_ratio))\n",
    "        \n",
    "        train_sequence = sequence[:train_end]\n",
    "        val_sequence = sequence[train_end:val_end]\n",
    "        test_sequence = sequence[val_end:]\n",
    "        \n",
    "        # 滑动窗口拆分序列，窗口大小为15\n",
    "        def split_sequence(seq):\n",
    "            result = []\n",
    "            start = 0\n",
    "            while start + 15 <= len(seq):\n",
    "                result.append(seq[start:start + 15])\n",
    "                start += 15\n",
    "            # 处理剩余部分\n",
    "            if start < len(seq):\n",
    "                result.append(seq[start:])\n",
    "            return result\n",
    "        \n",
    "        train_sequences = split_sequence(train_sequence)\n",
    "        val_sequences = split_sequence(val_sequence)\n",
    "        test_sequences = split_sequence(test_sequence)\n",
    "\n",
    "        for seq in train_sequences:\n",
    "            if len(seq) > 3:\n",
    "                train_data.append({\n",
    "                    'reviewerID_int': row['reviewerID_int'],\n",
    "                    'sequence': seq\n",
    "                })\n",
    "        for seq in val_sequences:\n",
    "            if len(seq) > 3:\n",
    "                val_data.append({\n",
    "                    'reviewerID_int': row['reviewerID_int'],\n",
    "                    'sequence': seq\n",
    "                })\n",
    "        for seq in test_sequences:\n",
    "            if len(seq) > 3:\n",
    "                test_data.append({\n",
    "                    'reviewerID_int': row['reviewerID_int'],\n",
    "                    'sequence': seq\n",
    "                })\n",
    "\n",
    "# 将结果保存到文本文件\n",
    "def save_sequence(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for item in data:\n",
    "            seq_str = '\\t'.join([f\"{x['asin_int']}|{x['timestamp']}|{datetime.datetime.fromtimestamp(x['timestamp']).strftime('%Y-%m-%d %H:%M:%S')}|\" for x in item['sequence']])\n",
    "            f.write(f\"{item['reviewerID_int']}\\t{len(item['sequence'])}\\t{seq_str}\\n\")\n",
    "\n",
    "save_sequence(train_data, 'dataset/Toy-Game/traindata.txt')\n",
    "save_sequence(val_data, 'dataset/Toy-Game/validdata.txt')\n",
    "save_sequence(test_data, 'dataset/Toy-Game/testdata.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy-Game LLM Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"处理元数据\"\"\"\n",
    "toy_meta = getDF('dataset/generate_LLM_embeddings/Toy-Game\\meta_Toys_and_Games.json.gz')\n",
    "game_meta = getDF('dataset/generate_LLM_embeddings/Toy-Game\\meta_Video_Games.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_item = pd.read_csv(\"dataset\\Toy-Game\\Alist.csv\")\n",
    "game_item = pd.read_csv(\"dataset\\Toy-Game\\Blist.csv\")\n",
    "df_toy = pd.merge(toy_item, toy_meta, on='asin', how='inner')\n",
    "df_game = pd.merge(game_item, game_meta, on='asin', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_toy = df_toy.drop_duplicates(subset='asin')\n",
    "df_game = df_game.drop_duplicates(subset='asin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='https://dashscope.aliyuncs.com/compatible-mode/v1',\n",
    "    api_key='sk-1bf40b7f0bdf41cf9e68509aa6dc1296'\n",
    ")\n",
    "\n",
    "def split_string(input_string, max_length=5000):\n",
    "    \"\"\"\n",
    "    将输入字符串拆分为最大长度为max_length的字符串列表\n",
    "    :param input_string: 需要拆分的字符串\n",
    "    :param max_length: 每个子字符串的最大长度，默认为5000\n",
    "    :return: 拆分后的字符串列表\n",
    "    \"\"\"\n",
    "    li = [input_string[i:i+max_length] for i in range(0, len(input_string), max_length)]\n",
    "    if len(li) > 8:\n",
    "        li = li[:8]\n",
    "    return li\n",
    "\n",
    "def get_embedding(df, asin):\n",
    "    # 查询特定的asin\n",
    "    item = df[df[\"asin\"] == asin]\n",
    "\n",
    "    if not item.empty:\n",
    "        title = item['title'].item()\n",
    "        category = item['category'].item()\n",
    "        description = item['description'].item()\n",
    "        text_desc = f'title: {title}\\ncategory: {category}\\ndescription: {description}'\n",
    "        response = client.embeddings.create(\n",
    "            input=split_string(text_desc, 4000),\n",
    "            model=\"text-embedding-v3\",\n",
    "            dimensions=1024,\n",
    "            encoding_format=\"float\"\n",
    "        )\n",
    "\n",
    "        return response.data[0].embedding\n",
    "    else:\n",
    "        return [0] * 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "embeddings = []\n",
    "part_number = 0\n",
    "total = len(game_item)\n",
    "\n",
    "for index, series in tqdm(enumerate(game_item.iterrows()), total=total):\n",
    "    embedding = get_embedding(df_game, series[1]['asin'])\n",
    "    embeddings.append([series[1]['asin_index'], series[1]['asin'], embedding])\n",
    "    \n",
    "    # 每 5000 次迭代保存一次中间结果\n",
    "    if (index + 1) % 5000 == 0:\n",
    "        part_df = pd.DataFrame(embeddings, columns=['id', 'asin', 'embedding'])\n",
    "        part_df.to_csv(f'dataset/game_embeddings_part_{part_number}.csv', index=False)\n",
    "        embeddings = []  # 重置 embeddings 列表\n",
    "        part_number += 1\n",
    "\n",
    "# 保存最后剩余的部分（如果有的话）\n",
    "if embeddings:\n",
    "    part_df = pd.DataFrame(embeddings, columns=['id', 'asin', 'embedding'])\n",
    "    part_df.to_csv(f'dataset/game_embeddings_part_{part_number}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并所有部分\n",
    "csv_files = [f for f in os.listdir('dataset') if f.startswith('game_embeddings_part_') and f.endswith('.csv')]\n",
    "csv_files.sort()  # 按照文件名排序\n",
    "\n",
    "# 读取所有中间文件并合并\n",
    "final_df = pd.concat([pd.read_csv(os.path.join('dataset', f)) for f in csv_files], ignore_index=True)\n",
    "\n",
    "# 保存最终的合并文件\n",
    "final_df.to_csv('dataset/game_embeddings_final.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "part_number = 13  # 假设已经保存到40000次\n",
    "total = len(toy_item)\n",
    "\n",
    "# 计算上次保存的索引位置\n",
    "last_index = 65000 - 1  # 40000次迭代对应40000条数据\n",
    "\n",
    "# 从上次保存的位置继续\n",
    "for index in tqdm(range(total), total=total):\n",
    "    if index <= last_index:\n",
    "        continue  # 跳过已经处理的条目\n",
    "    series = toy_item.iloc[index]\n",
    "    embedding = get_embedding(df_toy, series['asin'])\n",
    "    embeddings.append([series['asin_index'], series['asin'], embedding])\n",
    "    \n",
    "    # 每5000次迭代保存一次中间结果\n",
    "    if (index + 1) % 5000 == 0:\n",
    "        part_df = pd.DataFrame(embeddings, columns=['id', 'asin', 'embedding'])\n",
    "        part_df.to_csv(f'dataset/toy_embeddings_part_{part_number}.csv', index=False)\n",
    "        embeddings = []  # 重置embeddings列表\n",
    "        part_number += 1\n",
    "\n",
    "# 保存最后剩余的部分（如果有的话）\n",
    "if embeddings:\n",
    "    part_df = pd.DataFrame(embeddings, columns=['id', 'asin', 'embedding'])\n",
    "    part_df.to_csv(f'dataset/toy_embeddings_part_{part_number}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并所有部分\n",
    "csv_files = [f for f in os.listdir('dataset') if f.startswith('toy_embeddings_part_') and f.endswith('.csv')]\n",
    "csv_files.sort()  # 按照文件名排序\n",
    "\n",
    "# 读取所有中间文件并合并\n",
    "final_df = pd.concat([pd.read_csv(os.path.join('dataset', f)) for f in csv_files], ignore_index=True)\n",
    "\n",
    "# 保存最终的合并文件\n",
    "final_df.to_csv('dataset/toy_embeddings_final.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并所有部分\n",
    "csv_files = [f for f in os.listdir('dataset') if f.startswith('toy_embeddings_part_') and f.endswith('.csv')]\n",
    "csv_files.sort()  # 按照文件名排序\n",
    "\n",
    "# 读取所有中间文件并合并\n",
    "df_toy = pd.concat([pd.read_csv(os.path.join('dataset', f)) for f in csv_files], ignore_index=True)\n",
    "\n",
    "# 合并所有部分\n",
    "csv_files = [f for f in os.listdir('dataset') if f.startswith('game_embeddings_part_') and f.endswith('.csv')]\n",
    "csv_files.sort()  # 按照文件名排序\n",
    "\n",
    "# 读取所有中间文件并合并\n",
    "df_game = pd.concat([pd.read_csv(os.path.join('dataset', f)) for f in csv_files], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tqdm.pandas(desc=f\"Loading toy Embeddings\")\n",
    "df_toy['embedding_np'] = df_toy.embedding.progress_apply(lambda x: np.array(eval(x), dtype=np.float32))\n",
    "tqdm.pandas(desc=f\"Loading game Embeddings\")\n",
    "df_game['embedding_np'] = df_game.embedding.progress_apply(lambda x: np.array(eval(x), dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_x = np.vstack(df_toy['embedding_np'].to_numpy())\n",
    "array_y = np.vstack(df_game['embedding_np'].to_numpy())\n",
    "item_embeddings = np.vstack((array_x, array_y, np.zeros(1024, dtype=np.float32)))\n",
    "np.save(f'dataset/Toy-Game/item_embeddings.npy', item_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# all_user_ids = list(range(0, user_num))\n",
    "# # 找出缺失的用户ID\n",
    "# missing_user_ids = [uid for uid in all_user_ids if uid not in df_user['id'].values]\n",
    "# # 创建一个包含缺失用户ID的补充数据框\n",
    "# supplement_df = pd.DataFrame({'id': missing_user_ids, 'embedding': [[0]*1024]*len(missing_user_ids)})\n",
    "# # 将补充的数据框与原始数据框合并\n",
    "df_user = pd.concat([df_user, supplement_df]).sort_values(by='id').reset_index(drop=True)\n",
    "# array_x = np.vstack(df_x['embedding'].to_numpy())\n",
    "# array_y = np.vstack(df_y['embedding'].to_numpy())\n",
    "# item_embeddings = np.vstack((array_x, array_y, np.zeros(1024, dtype=np.float32)))\n",
    "user_embeddings = np.vstack(df_user['embedding_np'].to_numpy())\n",
    "# np.save(f'dataset/{domains}/item_embeddings.npy', item_embeddings)\n",
    "np.save(f'dataset/{domains}/reasoning_user_embeddings.npy', user_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从训练集中抽取所有用户交互，保留时间排序\n",
    "train_file = f'dataset/Toy-Game/traindata.txt'\n",
    "with codecs.open(train_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "    train_data = []\n",
    "    user_list = []\n",
    "    for id, line in enumerate(infile):\n",
    "        res = []\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        user_list.append(int(line[0]))\n",
    "\n",
    "        line = line[2:]  # 交互的一系列物品\n",
    "        for w in line:\n",
    "            w = w.split(\"|\")\n",
    "            res.append((int(w[0]), int(w[1])))\n",
    "        res.sort(key=lambda x: x[1])  # 按照时间顺序排列\n",
    "\n",
    "        res_2 = []\n",
    "        for r in res:\n",
    "            res_2.append(r[0])\n",
    "        train_data.append(res_2)\n",
    "user_interaction = dict()\n",
    "for user, data in zip(user_list, train_data):\n",
    "    user_interaction.setdefault(user, []).append(data)\n",
    "for user in user_interaction.keys():\n",
    "    user_interaction[user] = list(dict.fromkeys(sum(user_interaction[user], [])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_A = 'dataset/Toy-Game/Alist.csv'\n",
    "file_path_B = 'dataset/Toy-Game/Blist.csv'\n",
    "df_item_Alist = pd.read_csv(file_path_A)\n",
    "df_item_Blist = pd.read_csv(file_path_B)\n",
    "# item数量直接加和为两域之和\n",
    "df_item = pd.concat([df_item_Alist, df_item_Blist])\n",
    "df_meta_data_A = getDF('dataset/generate_LLM_embeddings/Toy-Game\\meta_Toys_and_Games.json.gz')\n",
    "df_meta_data_B = getDF('dataset/generate_LLM_embeddings/Toy-Game\\meta_Video_Games.json.gz')\n",
    "df_meta_data = pd.concat([df_meta_data_A, df_meta_data_B])\n",
    "df_text = pd.merge(df_item, df_meta_data, on='asin', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_request_data(custom_id, content):\n",
    "    \"\"\"\n",
    "    生成单个请求数据\n",
    "    :param custom_id: 自定义ID\n",
    "    :param content: 用户问题内容\n",
    "    :return: 请求数据的字典格式\n",
    "    \"\"\"\n",
    "    prompt = \"Below is a user's purchase record of items in the Toy-Game domains. Please note the temporal information: the higher the sequence number, the more recent the record. Use this sequence to characterize the user profile. The user profile should be concise and refined. And please avoid inappropriate content!\"\n",
    "    request_data = {\n",
    "        \"custom_id\": custom_id,\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"qwq-plus\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a recommendation expert, capable of profiling users based on their interaction records. The user profile should be concise and refined.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt+content}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    return request_data\n",
    "\n",
    "def get_interaction_metadata(df_text, user_interaction):\n",
    "    text = ''\n",
    "    for num, id in enumerate(user_interaction):\n",
    "        item = df_text.iloc[id]\n",
    "        title = str(item['title'])\n",
    "        category = str(item['category'])\n",
    "        text += f'{num+1}. ' + title + '  ' + category + '\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'dataset/440e74b0-6410-4463-a0fb-e15cd9e1f04c_1746977751857_error.jsonl'\n",
    "# 打开文件并逐行读取\n",
    "error_id = []\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # 解析每一行的 JSON 数据\n",
    "        data = json.loads(line)\n",
    "        # 获取 custom_id 和 content\n",
    "        custom_id = data.get('custom_id')\n",
    "        error_id.append(custom_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_id = [int(x) for x in error_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for id in user_interaction.keys():\n",
    "    # 使用原始id，确保交互列表中custom_id唯一\n",
    "    if id in error_id:\n",
    "        data_list.append(generate_request_data(id, get_interaction_metadata(df_text, user_interaction[id])))\n",
    "\n",
    "save_to_json_file(data_list, \"dataset/toy-game_user_interaction_2.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'dataset/toy-game_user_interaction_2.jsonl'\n",
    "# 打开文件并逐行读取\n",
    "data_list = []\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # 解析每一行的 JSON 数据\n",
    "        data = json.loads(line)\n",
    "        # 获取 custom_id 和 content\n",
    "        custom_id = data.get('custom_id')\n",
    "        content = data.get('body').get('messages')[1].get('content', '')\n",
    "        data_list.append((custom_id, content))\n",
    "df_user_profile = pd.DataFrame(data_list, columns=['id', 'user profile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'dataset/toy-game_user_profile.jsonl'\n",
    "# 打开文件并逐行读取\n",
    "data_list = []\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # 解析每一行的 JSON 数据\n",
    "        data = json.loads(line)\n",
    "        # 获取 custom_id 和 content\n",
    "        custom_id = data.get('custom_id')\n",
    "        content = data.get('response', {}).get('body', {}).get('choices', [{}])[0].get('message', {}).get('content', '')\n",
    "        data_list.append((custom_id, split_string(content)))\n",
    "df_user_profile = pd.DataFrame(data_list, columns=['id', 'user profile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_profile['id_int'] = df_user_profile['id'].apply(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "embeddings = []\n",
    "part_number = 0\n",
    "total = len(df_user_profile)\n",
    "i=0\n",
    "for index, series in tqdm(enumerate(df_user_profile.iterrows()), total=total):\n",
    "    embedding =  get_user_reasoning_embedding(series[1]['user profile'])\n",
    "    embeddings.append([series[1]['id'], embedding])\n",
    "    \n",
    "    # 每 5000 次迭代保存一次中间结果\n",
    "    if (index + 1) % 3000 == 0:\n",
    "        part_df = pd.DataFrame(embeddings, columns=['id', 'embedding'])\n",
    "        part_df.to_csv(f'dataset/user_embeddings_part_{part_number}.csv', index=False)\n",
    "        embeddings = []  # 重置 embeddings 列表\n",
    "        part_number += 1\n",
    "    i += 1\n",
    "\n",
    "# 保存最后剩余的部分（如果有的话）\n",
    "if embeddings:\n",
    "    part_df = pd.DataFrame(embeddings, columns=['id', 'embedding'])\n",
    "    part_df.to_csv(f'dataset/user_embeddings_part_{part_number}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 合并所有部分\n",
    "csv_files = [f for f in os.listdir('dataset') if f.startswith('user_embeddings_part_') and f.endswith('.csv')]\n",
    "csv_files.sort()  # 按照文件名排序\n",
    "\n",
    "# 读取所有中间文件并合并\n",
    "df_user = pd.concat([pd.read_csv(os.path.join('dataset', f)) for f in csv_files], ignore_index=True)\n",
    "df_user['embedding_np'] = df_user.embedding.apply(lambda x: np.array(eval(x), dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_ids = list(range(0, 15574))\n",
    "# 找出缺失的用户ID\n",
    "missing_user_ids = [uid for uid in all_user_ids if uid not in df_user['id'].values]\n",
    "# 创建一个包含缺失用户ID的补充数据框\n",
    "supplement_df = pd.DataFrame({'id': missing_user_ids, 'embedding_np': [np.zeros([1024], dtype=np.float32)]*len(missing_user_ids)})\n",
    "# supplement_df['embedding_np'] = supplement_df['embedding'].apply(lambda x: np.array(x, dtype=np.float32).squeeze())\n",
    "# # 将补充的数据框与原始数据框合并\n",
    "df_user = pd.concat([df_user, supplement_df]).sort_values(by='id').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user['embedding_np'].iloc[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeddings = np.vstack(df_user['embedding_np'].to_numpy())\n",
    "# np.save(f'dataset/{domains}/item_embeddings.npy', item_embeddings)\n",
    "np.save(f'dataset/Toy-Game/reasoning_user_embeddings.npy', user_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
